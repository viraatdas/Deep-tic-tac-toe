{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "Self-learning Tic Tac Toe\n",
        "Made by Lorenzo Mambretti and Hariharan Sezhiyan\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "class State:\n",
        "    board = np.zeros((3,3))\n",
        "    terminal = False\n",
        "\n",
        "def is_valid(action, state):\n",
        "    if state.board[int(np.floor(action / 3))][action % 3] != 0:\n",
        "        return False\n",
        "    else:\n",
        "        return True\n",
        "\n",
        "def step(state, action):\n",
        "\n",
        "    # insert\n",
        "    state_ = State()\n",
        "    state_.board = np.copy(state.board)\n",
        "    row_index = int(np.floor(action / 3))\n",
        "    col_index = action % 3\n",
        "    state_.board[row_index][col_index] = 1\n",
        "\n",
        "    # undecided\n",
        "    terminal = 1\n",
        "\n",
        "    # to check for 3 in a row horizontal\n",
        "    for row in range(3):\n",
        "        for col in range(3):\n",
        "            if(state_.board[row][col] != 1):\n",
        "                terminal = 0\n",
        "        if(terminal == 1):\n",
        "            state_.terminal = True\n",
        "            return +1, state_\n",
        "        else:\n",
        "            terminal = 1\n",
        "\n",
        "    # to check for 3 in a row vertical\n",
        "    for col in range(3):\n",
        "        for row in range(3):\n",
        "            if(state_.board[row][col] != 1):\n",
        "                terminal = 0\n",
        "        if(terminal == 1):\n",
        "            state_.terminal = True\n",
        "            return +1, state_\n",
        "        else:\n",
        "            terminal = 1\n",
        "\n",
        "    # diagonal top-left to bottom-right\n",
        "    for diag in range(3):\n",
        "        if(state_.board[diag][diag] != 1):\n",
        "            terminal = 0\n",
        "    if(terminal == 1):\n",
        "        state_.terminal = True\n",
        "        return +1, state_\n",
        "    else:\n",
        "        terminal = 1\n",
        "\n",
        "    # diagonal bottom-left to top-right\n",
        "    for diag in range(3):\n",
        "        if(state_.board[2 - diag][diag] != 1):\n",
        "            terminal = 0\n",
        "    if(terminal == 1):\n",
        "        state_.terminal = True\n",
        "        return +1, state_\n",
        "    else:\n",
        "        terminal = 1\n",
        "\n",
        "    # checks if board is filled completely\n",
        "    for row in range(3):\n",
        "        for col in range(3):\n",
        "            if(state_.board[row][col] == 0):\n",
        "                terminal = 0\n",
        "                break\n",
        "    if terminal == 1:\n",
        "        state_.terminal = True\n",
        "\n",
        "    return 0, state_\n",
        "\n",
        "def save(W1, W2, B1, B2):\n",
        "    np.savez(\"weights.npz\", W1, W2, B1, B2)\n",
        "    print(\"file weights.txt has beeen updated successfully\")\n",
        "\n",
        "def load():\n",
        "    npzfile = np.load(\"weights.npz\")\n",
        "    W1 = np.reshape(npzfile['arr_0'], (27, 18))\n",
        "    W2 = np.reshape(npzfile['arr_1'], (18,9))\n",
        "    b1 = np.reshape(npzfile['arr_2'], (18))\n",
        "    b2 = np.reshape(npzfile['arr_3'], (9))\n",
        "    return w1, w2, b1, b2\n",
        "\n",
        "def extract_policy(state):\n",
        "    policy = None\n",
        "    q_values = compute_Q_values(state)\n",
        "    for action in range(9):\n",
        "        if is_valid(action,state):\n",
        "            if policy == None:\n",
        "                policy = action\n",
        "                best_q = q_values[action]\n",
        "            else:\n",
        "                new_q = q_values[action]\n",
        "                if new_q > best_q:\n",
        "                    policy = action\n",
        "                    best_q = new_q\n",
        "    return policy\n",
        "    \n",
        "def invert_board(state):\n",
        "    state_ = State()\n",
        "    state_.board = np.copy(state.board)\n",
        "    state_.terminal = state.terminal\n",
        "    for row in range(3):\n",
        "        for col in range(3):\n",
        "            if(state.board[row][col] == 1):\n",
        "                state_.board[row][col] = 2\n",
        "            elif(state.board[row][col] == 2):\n",
        "                state_.board[row][col] = 1\n",
        "\n",
        "    return state_\n",
        "\n",
        "def play_game():\n",
        "    while(True):\n",
        "        start_nb = input(\"If you would like to move first, enter 1. Otherwise, enter 2. \")\n",
        "        start = int(start_nb)\n",
        "        state = State()\n",
        "        state.board = np.zeros((3,3))\n",
        "\n",
        "        while not state.terminal:\n",
        "            if start == 1:\n",
        "                action = int(input(\"Please enter your move: \"))\n",
        "                while(is_valid(action, state) == False):\n",
        "                    action = int(input(\"Please enter a correct move: \"))\n",
        "                start = 0\n",
        "                r, state = step(state, action)\n",
        "            else:\n",
        "                state = invert_board(state)\n",
        "                action = extract_policy(state)\n",
        "                start = 1\n",
        "                r, state = step(state, action)\n",
        "                r = -r\n",
        "                state = invert_board(state)\n",
        "\n",
        "            print(state.board)\n",
        "                \n",
        "        if r == 0:\n",
        "            print (\"Tie\")\n",
        "        elif r == 1:\n",
        "            print (\"You won\")\n",
        "        else:\n",
        "            print (\"You lost\")\n",
        "\n",
        "def convert_state_representation(state):\n",
        "    new_board = np.zeros(27)\n",
        "    for row in range(3):\n",
        "        for col in range(3):\n",
        "            if(state[row][col] == 0):\n",
        "                new_board[9 * row + 3 * col] = 1\n",
        "            elif(state[row][col] == 1):\n",
        "                new_board[9 * row + 3 * col + 1] = 1\n",
        "            else:\n",
        "                new_board[9 * row + 3 * col + 2] = 1\n",
        "\n",
        "    return(new_board)\n",
        "\n",
        "def compute_Q_values(state):\n",
        "    # computes associated Q value based on NN function approximator\n",
        "    q_board = np.zeros((1,27))\n",
        "    q_board = [np.copy(convert_state_representation(state.board))]\n",
        "\n",
        "    #NN forward propogation\n",
        "    q_values = sess.run(y, feed_dict = {x: q_board})\n",
        "    q_values = np.reshape(q_values, 9)\n",
        "    return (q_values)\n",
        "\n",
        "def train(experience_replay, saved_W1, saved_W2, saved_b1, saved_b2):\n",
        "    # can modify batch size here\n",
        "    batch_size = 32\n",
        "\n",
        "    # take a random mini_batch\n",
        "    mini_batch = experience_replay[np.random.choice(experience_replay.shape[0], batch_size), :]\n",
        "\n",
        "    # select state, state_, action, and reward from the mini batch\n",
        "    state = np.concatenate(mini_batch[:,0]).reshape((batch_size, -1))\n",
        "    act = np.array(mini_batch[:,1])\n",
        "    act = np.append([np.arange(batch_size)],[act], axis = 0)\n",
        "    act = np.transpose(act)\n",
        "    r = mini_batch[:,2]\n",
        "    state_ = np.concatenate(mini_batch[:,3]).reshape((batch_size, -1))\n",
        "    done = mini_batch[:,4]\n",
        "    \n",
        "    # is the list of all rewards within the mini_batch\n",
        "        \n",
        "    summary, _= sess.run([merged, train_step], feed_dict={  x: state,\n",
        "                                                            x_old : state_,\n",
        "                                                            W1_old : saved_W1,\n",
        "                                                            W2_old : saved_W2,\n",
        "                                                            b1_old : saved_b1,\n",
        "                                                            b2_old : saved_b2,\n",
        "                                                            l_done : done,\n",
        "                                                            reward : r, \n",
        "                                                            action_t : act\n",
        "                                                            })\n",
        "    train_writer.add_summary(summary)\n",
        "\n",
        "# Q learner neural network\n",
        "with tf.name_scope('Q-learner') as scope:\n",
        "    x = tf.placeholder(tf.float32, [None, 27], name='x')\n",
        "    with tf.name_scope('hidden_layer') as scope:\n",
        "        W1 = tf.get_variable(\"W1\", shape=[27, 18],\n",
        "           initializer=tf.contrib.layers.xavier_initializer())\n",
        "        b1 = tf.get_variable(\"b1\", shape=[18],\n",
        "           initializer=tf.contrib.layers.xavier_initializer())\n",
        "        h1 = tf.tanh(tf.matmul(x, W1) + b1)\n",
        "    with tf.name_scope('output_layer') as scope:\n",
        "        W2 = tf.get_variable(\"W2\", shape=[18, 9],\n",
        "           initializer=tf.contrib.layers.xavier_initializer())\n",
        "        b2 = tf.get_variable(\"b2\", shape=[9],\n",
        "           initializer=tf.contrib.layers.xavier_initializer())\n",
        "        y = tf.tanh(tf.matmul(h1, W2) + b2)\n",
        "        action_t = tf.placeholder(tf.int32, [None, 2])\n",
        "\n",
        "    q_learner = tf.gather_nd(y, action_t)\n",
        "\n",
        "# Q target neural network\n",
        "with tf.name_scope('Q-target') as scope:\n",
        "    x_old = tf.placeholder(tf.float32, [None, 27], name='x_old')\n",
        "    with tf.name_scope('hidden_layer') as scope:\n",
        "        W1_old = tf.placeholder(tf.float32, [27, 18], name='W1_old')\n",
        "        b1_old = tf.placeholder(tf.float32, [18], name='b1_old')\n",
        "        h1_old = tf.tanh(tf.matmul(x_old, W1_old) + b1_old, name='h1')\n",
        "    with tf.name_scope('output_layer') as scope:\n",
        "        W2_old =tf.placeholder(tf.float32, [18, 9], name='W2_old')\n",
        "        b2_old =tf.placeholder(tf.float32, [9], name='b2_old')\n",
        "        y_old = tf.tanh(tf.matmul(h1_old, W2_old) + b2_old, name='y_old')\n",
        "\n",
        "    l_done = tf.placeholder(tf.bool, [None])\n",
        "    reward = tf.placeholder(tf.float32, [None])\n",
        "    gamma = tf.constant(0.99, name='gamma')\n",
        "    qt = tf.reduce_max(y_old, axis = 1, name='maximum_qt')\n",
        "    q_target = tf.where(l_done, reward, reward + (gamma * qt), name='selected_max_qt')\n",
        "\n",
        "with tf.name_scope('loss') as scope:\n",
        "    loss = tf.losses.mean_squared_error(q_target, q_learner)\n",
        "    \n",
        "#train_step = tf.train.GradientDescentOptimizer(0.03).minimize(loss)\n",
        "train_step = tf.train.RMSPropOptimizer(0.00025, momentum=0.95, use_locking=False, centered=False, name='RMSProp').minimize(loss)\n",
        "\n",
        "tf.summary.scalar('loss', loss)\n",
        "merged = tf.summary.merge_all()\n",
        "\n",
        "sess = tf.InteractiveSession()\n",
        "train_writer = tf.summary.FileWriter('tensorflow_logs', sess.graph)\n",
        "tf.global_variables_initializer().run()\n",
        "\n",
        "episodes = 100000\n",
        "n0 = 100.0\n",
        "start_size = 500\n",
        "experience_replay = np.zeros((0,5))\n",
        "\n",
        "print(\"All set. Start epoch\")\n",
        "\n",
        "for e in range(episodes):\n",
        "    # print(\"episode \",e)\n",
        "    state = State()\n",
        "    if e >= start_size:\n",
        "        epsilon = max(n0 / (n0 + (e- start_size)), 0.1)\n",
        "    else: epsilon = 1\n",
        "    \n",
        "    if e % 2 == 1:\n",
        "        # this is player 2's turn\n",
        "            state = invert_board(state)\n",
        "            if random.random() < epsilon:\n",
        "                # take random action\n",
        "                action_pool = np.random.choice(9,9, replace = False)\n",
        "                for a in action_pool:\n",
        "                    if is_valid(a, state):\n",
        "                        action = a\n",
        "                        break\n",
        "            else:\n",
        "                # take greedy action\n",
        "                action = extract_policy(state)\n",
        "\n",
        "            r, state = step(state, action)\n",
        "            state = invert_board(state)\n",
        "            r = -r \n",
        "    \n",
        "    while not state.terminal:\n",
        "        # this section is player 1's turn\n",
        "        # select epsilon-greedy action\n",
        "        if random.random() < epsilon:\n",
        "            # take random action\n",
        "            action_pool = np.random.choice(9,9, replace = False)\n",
        "            for a in action_pool:\n",
        "                if is_valid(a, state):\n",
        "                    action = a\n",
        "                    break\n",
        "        else:\n",
        "            # take greedy action\n",
        "            action = extract_policy(state)\n",
        "\n",
        "        r, state_ = step(state, action)\n",
        "\n",
        "        if not state_.terminal:\n",
        "            # this is player 2's turn\n",
        "            state_ = invert_board(state_)\n",
        "            if random.random() < epsilon:\n",
        "                # take random action\n",
        "                action_pool = np.random.choice(9,9, replace = False)\n",
        "                for a in action_pool:\n",
        "                    if is_valid(a, state_):\n",
        "                        action2 = a\n",
        "                        break\n",
        "            else:\n",
        "                # take greedy action\n",
        "                action2 = extract_policy(state_)\n",
        "\n",
        "            r, state_ = step(state_, action2)\n",
        "            state_ = invert_board(state_)\n",
        "            r = -r \n",
        "\n",
        "        s = convert_state_representation(np.copy(state.board))\n",
        "        s_ = convert_state_representation(np.copy(state_.board))\n",
        "        done = state_.terminal\n",
        "        D = (s, action, r, s_, done)\n",
        "        experience_replay = np.append(experience_replay, [D], axis = 0)\n",
        "        state.board = np.copy(state_.board)\n",
        "        state.terminal = state_.terminal\n",
        "\n",
        "    if e == start_size: print(\"Start Training\")\n",
        "    if e >= start_size:\n",
        "        if((e % 50) == 0):\n",
        "            print(\"Episode:\",e)\n",
        "            # here save the W1,W2,b1,B2\n",
        "            saved_W1 = W1.eval()\n",
        "            saved_W2 = W2.eval()\n",
        "            saved_b1 = b1.eval()\n",
        "            saved_b2 = b2.eval()\n",
        "        train(experience_replay, saved_W1, saved_W2, saved_b1, saved_b2)\n",
        "print(\"Training completed\")\n",
        "play_game()\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}